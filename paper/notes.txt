## Replication Paper Kynigakis & Panopoulou ##

- study examines whether return forcasts generated by linear and nonlinear 
ML methods and their combination benefit portfolios when compared with
simple forecast combinations (EW & HA)

- specific literature relation: Callot et al., 2019 & D´Hondt et al., 2020 
Contribution 1: investigating the benefits of integrating return 
forecasts from ML methodologies into an out-of-sample PO framework
Contribution 2:exploring the economic value of a wide range of ML methods
when used to model returns as inputs to asset allocation

Paper employs a variety of ML methods along with forecast combination 
schemes to generate the return forecasts.
- to explore the potential benefits of using ML methods in an asset allocation
setting, construct portfolios based on the return forecasts generated 
from the MV predicition models
- compare out-of-sample performance utilizing ML forecasts to that of
the EW portfolio and a MV portfolio based on HA


RETURN PREDICTION MODELS

- commonly used model is the classic linear regression model 
estimated by OLS: f(x) = a + bX
(known to have poor forecasting performance, as the the estimated parameters
have low bias but high variance)

1. Shrinkage models
https://www.datasklr.com/extensions-of-ols-regression/regularization-and-shrinkage-ridge-lasso-and-elastic-net-regression

- used to address the problem of overfitting in predictive models
- overfitting occurs when a model fits too closely to the training data 
and fails to generalize well to new, unseen data
- this often happens when a model has too many parameters relative to 
the amount of available training data

See formula (5) in Paper for penalty function
- lambda, tuning parameter which is determined seperately and controls
the amount of shrinkage
- gamma, hyperparameter that controlls the trade-off between l1 and l2
regularization
- alpha, hyperparameter that controls the strength of the weight w

1.1. RIDGE: alpha=0, gamma=0; shrinks the coefficients towards zero
1.2. LASSO: alpha=0, gamma=1; allows for both shrinkage and variable selection
1.3. ELASTIC NET: alpha=0, gamma e{0,1} ;combines them and thus 
performing continuous shrinkage and automatic variable selection
1.4. ADAPTIVE LASSO: alpha>0, gamma=1; modifying lasso to include adaptive
weights that penalize individual coefficients less severely
1.5. BRIDGE: bridge penalty term represents all the penalties between 
ridge regression and best-subsets selection

Further they use smoothly clipped absolute deviation (SCAD),
minimax concave penalty (MCP) and 
smooth integration of counting and absolute deviation (SICA)

Sklearn offer corresponding modules:
linear_model.Ridge, linear_model.Lasso, linear_model.ElasticNet
linear_model.LassoLarsCV ('LassoLarsIC' criterion set to 'bc' or 'aic')

2. DIMENSIONALITY REDUCTION METHODS
https://www.spiceworks.com/tech/artificial-intelligence/articles/what-is-dimensionality-reduction/

- method of reducing variables in a training dataset to develop ML models
- projecting high dimensional data to a lower dimensional space 

Example: train a model that can forecast the next day’s weather based 
on the current climatic variables such as the amount of sunlight, 
rainfall, temperature, humidity, and several other environmental factors.
Analyzing all these variables is a complex and challenging task. Hence, 
to accomplish the task with a limited set of features, you can target only 
specific features that show a stronger correlation and can be clubbed into one.
For instance, we can combine the humidity and temperature variables 
into one dependent feature as they tend to show a stronger correlation.

2.1. PARTIAL LEAST SQUARES:identifies the features in a superivised way
2.2. SPARSE PARTIAL LEAST SQUARES: extension of PLS that imposes l1 penalty
tp promote sparsity onto a surfroagte weight vector instead of the original
weight vector
2.3. PRINCIPAL COMPONENT ANALYSIS: most widely used method to obtain 
estimates of the latent factors; goal is to find the first K principal 
component weight vectors
2.4. SPARSE PRINCIPAL COMPONENT ANALYSIS: produces midified principal 
components with sparse weights s.t. each principal component is a linear
combination of only a few of the original predictors
2.5. INDEPENDET COMPONENT ANALYSIS: aims at finding a linear representation
of non-Gaussian data so that the components are statistically independent


Sklearn offers corresponding modules:
cross_decomposition.PLSRegression, cross_decomposition.PLSSVD,
decomposition.PCA, decomposition.SparsePCA, decomposition.FASTICA


















Recap:
- Supervised ML learning: model trained on a labeled dataset, where 
the target variable is known for each training example; goal is to learn 
a mapping function from input to output to predict for unseen data
- Unsupervised ML learning: model is trained on unlabeled dataset,
where target variable is not known; goal is to find patterns or structure
